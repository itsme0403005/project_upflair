{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " problem -> data collection ->data cleaning->eda->transorm->model train->model evaluation ->testing->  deployment ->database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " one hot encoding used for two class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " label encoding used for multiple class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression \n",
    "Linear Regression is a simple but powerful technique to model relationships between a dependent variable (y) and one or more independent variables (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression :- Used when predicting continuous values (e.g., house prices, temperature).\n",
    "1. MSE mean square error :\n",
    "a. formula:1/n E(y actal -y pred)^2\n",
    "b. disadvantage:-outlier\n",
    "2. MAE mean absolute error : consider outliers and calculate loss\n",
    "a. formula :1/n E|y actal -y pred|\n",
    "b. disadvantage: gradient decent\n",
    "3. RMSE root mean sq error : \n",
    "a. formula : root(1/n E(y actal -y pred)^2)\n",
    "4. Huber loss : combine of MAE and MSE\n",
    "a. formula : MAE + MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Loss Functions:-Used when predicting categories (e.g., spam or not spam).\n",
    "\n",
    "1. Cross-Entropy Loss (Log Loss) : Used in classification (Softmax/Sigmoid).\n",
    "2. Hinge Loss : Used for SVM models.\n",
    "3. Binary Cross-Entropy : For binary classification (Yes/No).\n",
    "4. Categorical Cross-Entropy : For multi-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom & Advanced Loss Functions :-\n",
    "1. KL Divergence : Measures difference between two probability distributions.\n",
    "2. Triplet Loss : Used in face recognition models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. score\n",
    "2. accuracy\n",
    "3. recall\n",
    "4. confusion matrix\n",
    "5. precision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Derived from the Confusion Matrix\n",
    "structure of a Confusion Matrix\n",
    "For a binary classification problem (e.g., spam vs. not spam), the confusion matrix looks like this:\n",
    "\n",
    "                            Actual Positive (1)\t         Actual Negative (0)\n",
    "\n",
    "    Predicted Positive (1)  True Positive (TP)\t         False Positive (FP)\n",
    "\n",
    "    Predicted Negative (0)\t False Negative (FN)\t        True Negative (TN)\n",
    "\n",
    "TP : MODEL: +ve and ACTUAL : +ve\n",
    "\n",
    "TN : MODEL: -ve and ACTUAL : -ve\n",
    "\n",
    "FN (TYPR 1 ERROR): MODEL: -ve and ACTUAL : +ve\n",
    "\n",
    "FP (TYPE 2 ERROR): MODEL:+ve and ACTUAL : -ve\n",
    "\n",
    "Using the confusion matrix, we can calculate several performance metrics:\n",
    "1. Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "Measures overall correctness of the model.\n",
    "\n",
    "2. Precision (Positive Predictive Value) = TP / (TP + FP)\n",
    "\n",
    "How many predicted positives are actually positive?\n",
    "\n",
    "3. Recall (Sensitivity, True Positive Rate) = TP / (TP + FN)\n",
    "\n",
    "How many actual positives were correctly predicted?\n",
    "\n",
    "4. F1-Score = 2 × (Precision × Recall) / (Precision + Recall)\n",
    "\n",
    "Harmonic mean of precision and recall.\n",
    "\n",
    "5. False Positive Rate (FPR) = FP / (FP + TN)\n",
    "\n",
    "Probability of incorrectly classifying a negative as positive.\n",
    "\n",
    "6. False Negative Rate (FNR) = FN / (FN + TP)\n",
    "\n",
    "Probability of missing a positive case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine :-\n",
    "* supervised learnind\n",
    "* works on both classification and regression\n",
    "* hyper plane \n",
    "* distance formula\n",
    "* support vector\n",
    "* types :-\n",
    "1. linear \n",
    "2. non linear : convet into cluster or filter it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN :-\n",
    "* supervised learning\n",
    "* work on both classification and regression\n",
    "* k nearest neighbour\n",
    "* distance formula\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "* A Decision Tree is a supervised learning algorithm used for both classification and regression tasks. \n",
    "* It splits data into branches based on feature values to make decisions.\n",
    "* entropy (order data)\n",
    "* gini entropy\n",
    " (unorder data)\n",
    "* information gain\n",
    "* terminology : 1. root node  2.splitting  3. branch  4. decision node   5.leaf node "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* world of the crowd \n",
    "* multiple model train\n",
    "* types: \n",
    "1. voting : diff algo->model->result\n",
    "2. bagging (horizontal): same algo->model same->result  algo random forest\n",
    "3. boosting (vertical): same algo->model same->result   \n",
    " * adaboost: weak learn\n",
    "   * step : split> wt assign> total error > aplha >update wt >normalize> bin assign > final result\n",
    " * xgboosting  : based on residual \n",
    "   *  avg > residual >create dt (-,+) > find similarity > info gain >log odds > pass throgh sigmoid(binary)\n",
    " * gradient boosting :\n",
    "  * weak learner\n",
    "  * leaf node(8-32)\n",
    "    * regression (mse) : avg of y add new feature y avg > residual(actual-pred avg) add new feature r1 >residual pred(r1p1) >updaate the y_avg =y_avg+(lr*r1p1) > avg \n",
    "    * classification ( log loss function): first model work as simple regg andd other  as gradient calculate log loss >conert probability> residual\n",
    "    \n",
    "4. stacking : diff algo->model->result->dataframe->algo->result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest\n",
    "* supervised\n",
    "* classification and regression\n",
    "* overfitting\n",
    "* weakless training\n",
    "* parameter : a. estimater (approach)     b.n_estimator(default 100)\n",
    "c. oob score(out of bag):validation       d.oob error(out of bag): validation error(1-oob score) default true\n",
    "* hyperparameter : a. n_estimator                b. criterian : entropy , gini , info gain\n",
    "                   c. max depth (default : 3)    d.min sample split:2                    \n",
    "                   e. min sample leaf            f. oob score   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparamter turning\n",
    " * techinque :\n",
    "  1. Grid search cv\n",
    "  2. random search cv\n",
    "  3. bayesian optimization\n",
    "  4. genetic algo\n",
    "  5. hyberband"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# naive bayes theorm\n",
    " * classification\n",
    " * feature dependent\n",
    " * bayes theorm\n",
    " * type:\n",
    "    * bernolli (for binary)\n",
    "    * gaussian (multiple outcome)\n",
    "    * multimonial"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
